{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import transformers\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Optimizer\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "from statistics import mode\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "D8DvnzD7DP0M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mount_directory = \"/content/drive\"\n",
        "dataset_path = os.path.join(mount_directory, 'MyDrive/MELD/MELD_train_efr.json')\n",
        "model_card = 'bert-base-uncased'\n",
        "drive.mount(mount_directory)\n",
        "initial_seed = 2\n",
        "seeds = [42, 53, 146, 34, 21]\n",
        "batch_size = 2\n",
        "patience = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTPAnY4jWukx",
        "outputId": "af34343f-19f1-40c8-c9bc-f67c0f05c1a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_onehotencoder(data: pd.core.series.Series) -> LabelBinarizer:\n",
        "  onehotencoder = LabelBinarizer()\n",
        "  data_flattened = np.concatenate(data.values)\n",
        "  onehotencoder.fit(data_flattened)\n",
        "  return onehotencoder"
      ],
      "metadata": {
        "id": "ENzi7RazbZeU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sbagliato, deve esserci un tokentypeids per ogni token e deve essere riferito a uno speaker particolare\n",
        "def map_to_order_of_occurrence(data: list) -> list:\n",
        "    # When a new entry is added, its value is computed using the lambda function\n",
        "    dict_order_of_occurrence = defaultdict(lambda: len(dict_order_of_occurrence)+1)\n",
        "    order_of_occurrence = [dict_order_of_occurrence[element] for element in data]\n",
        "    return order_of_occurrence"
      ],
      "metadata": {
        "id": "Pp1Tfr8PAVU8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_none_with_zero(data: list) -> list:\n",
        "    return [0 if x is None else x for x in data]"
      ],
      "metadata": {
        "id": "Qqer3ZIOE-td"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dialogue(dialogue_text: list, tokenizer,  speakers = None) -> list:\n",
        "    if speakers!=None:\n",
        "      tokenized_dialogue = [[speakers[idx]] + tokenizer.tokenize(utterance) + [tokenizer.sep_token] for idx, utterance in enumerate(dialogue_text)]\n",
        "    else:\n",
        "      tokenized_dialogue = [tokenizer.tokenize(utterance) + [tokenizer.sep_token] for utterance in dialogue_text]\n",
        "    tokenized_dialogue.insert(0, [tokenizer.cls_token])\n",
        "    flattened_tokens = [token for sublist in tokenized_dialogue for token in sublist]\n",
        "    return flattened_tokens"
      ],
      "metadata": {
        "id": "ktvQEsNPE618"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding(x: list, max_x_length: int, pad_value, pad_length = None) -> list:\n",
        "    x_length = len(x)\n",
        "    num_pad_values = max_x_length - x_length\n",
        "    if pad_length == None:\n",
        "      padded_x = x + [pad_value] * num_pad_values\n",
        "    else:\n",
        "      pad_list = [pad_value] * pad_length\n",
        "      pad_lists = np.tile(pad_list, (num_pad_values, 1))\n",
        "      padded_x = np.concatenate((x, pad_lists), axis=0)\n",
        "    return padded_x"
      ],
      "metadata": {
        "id": "oA4NVL4vOd6k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_alternative_dialogue(utterances: pd.core.series.Series, speakers: pd.core.series.Series, tokenizer) -> pd.core.series.Series:\n",
        "  dialogue_column = []\n",
        "  for dialogue_index, dialogue in utterances.items():\n",
        "    dialogue_column.append(tokenize_dialogue(dialogue, tokenizer, speakers[dialogue_index]))\n",
        "  return pd.Series(dialogue_column)"
      ],
      "metadata": {
        "id": "AuDGLC_mioTI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataframes(df, emotions_encoder, tokenizer):\n",
        "    column_names = ['emotions', 'speakers', 'triggers', 'dialogues', 'dialogues_ids', 'attention_masks']\n",
        "    df_standard = pd.DataFrame(columns = column_names)\n",
        "    df_standard['speakers'] = df['speakers'].apply(lambda x: map_to_order_of_occurrence(x))\n",
        "    df_standard['emotions'] = df['emotions'].apply(lambda x: emotions_encoder.transform(x))\n",
        "    df_standard['triggers'] = df['triggers'].apply(lambda x: replace_none_with_zero(x))\n",
        "    df_variation = copy.deepcopy(df_standard)\n",
        "    df_standard['dialogues'] = df['utterances'].apply(lambda x: tokenize_dialogue(x, tokenizer))\n",
        "    df_variation['dialogues'] = build_alternative_dialogue(df['utterances'], df_standard['speakers'], tokenizer)\n",
        "    return df_standard, df_variation"
      ],
      "metadata": {
        "id": "-lTKm0ZnaRzq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe(df):\n",
        "    _folder = Path.cwd().joinpath(\"dataframes\")\n",
        "    if not _folder.exists():\n",
        "        _folder.mkdir(parents=True)\n",
        "\n",
        "    df_path = Path.joinpath(_folder, 'df_MELD_efr'+'.pkl')\n",
        "    df.to_pickle(df_path)"
      ],
      "metadata": {
        "id": "5g6CwpOlYgGN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataframe():\n",
        "    df_path = 'dataframes/df_MELD_efr.pkl'\n",
        "    if not os.path.exists(df_path):\n",
        "        raise FileNotFoundError(\"{0} dataframe does not exist!\".format(df_path))\n",
        "\n",
        "    with open(df_path, 'rb') as file:\n",
        "        df = pickle.load(file)\n",
        "    return df"
      ],
      "metadata": {
        "id": "6mJwYXvlYlMK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataframe(orginal_df: pd.DataFrame, seed: int):\n",
        "    #train, test_validation = train_test_split(orginal_df, test_size=0.2, random_state=seed)\n",
        "    train, test_validation = train_test_split(orginal_df, test_size=0.2, random_state=seed)\n",
        "    validation, test = train_test_split(test_validation, test_size=0.5, random_state=seed)\n",
        "    return train.reset_index(drop=True), validation.reset_index(drop=True), test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "keNSPuCNqcWR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, speakers: pd.core.series.Series,\n",
        "                 dialogues:  pd.core.series.Series,\n",
        "                 emotions:  pd.core.series.Series,\n",
        "                 triggers:  pd.core.series.Series,\n",
        "                 device,\n",
        "                 pad_token: str,\n",
        "                 max_num_utterances):\n",
        "        self.max_dialogue_length = dialogues.apply(len).max()\n",
        "        self.max_num_utterances = max_num_utterances\n",
        "        self.dialogues = dialogues.apply(lambda x: add_padding(x, self.max_dialogue_length, pad_token))\n",
        "        self.dialogues_ids = self.dialogues.apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
        "        self.attention_masks = self.dialogues.apply(lambda x: [1 if token != pad_token else 0 for token in x])\n",
        "        self.speakers = speakers.apply(lambda x: add_padding(x, self.max_num_utterances, pad_value=0))\n",
        "        num_emotion_classes = len(emotions[0][0])\n",
        "        self.emotions = emotions.apply(lambda x: add_padding(x, self.max_num_utterances, pad_value = 0, pad_length = num_emotion_classes))\n",
        "        self.triggers = triggers.apply(lambda x: add_padding(x, self.max_num_utterances, pad_value=0))\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        speakers = torch.tensor(self.speakers.iloc[idx], dtype=torch.long).to(device)\n",
        "        dialogues_ids = torch.tensor(self.dialogues_ids.iloc[idx], dtype=torch.long).to(device)\n",
        "        dialogues_masks =  torch.tensor(self.attention_masks.iloc[idx], dtype=torch.long).to(device)\n",
        "        emotions = torch.tensor(self.emotions.iloc[idx], dtype=torch.float32).to(device)\n",
        "        triggers = torch.tensor(self.triggers.iloc[idx], dtype=torch.float32).to(device)\n",
        "        return  speakers, dialogues_ids, dialogues_masks, emotions, triggers"
      ],
      "metadata": {
        "id": "qrxDlf7pxa4d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(df: pd.core.frame.DataFrame, device, tokenizer, max_num_utterances, batch_size) -> torch.utils.data.dataloader.DataLoader :\n",
        "    dataset =  CustomDataset(speakers = df['speakers'],\n",
        "                                dialogues = df['dialogues'],\n",
        "                                emotions = df['emotions'],\n",
        "                                triggers = df['triggers'],\n",
        "                                device = device,\n",
        "                                pad_token = tokenizer.pad_token,\n",
        "                                max_num_utterances = max_num_utterances)\n",
        "    return DataLoader(dataset, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "1RfNKM1VqIHD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractModel(ABC, torch.nn.Module):\n",
        "   def __init__(self, num_emotions, max_num_utterances, device):\n",
        "        super(AbstractModel, self).__init__()\n",
        "        self.num_emotions = num_emotions\n",
        "        self.max_num_utterances =max_num_utterances\n",
        "        self.device = device\n",
        "\n",
        "   @abstractmethod\n",
        "   def forward(self, token_type_ids, input_ids, attention_mask, emotions, triggers):\n",
        "        pass"
      ],
      "metadata": {
        "id": "hHWN8RzDX24O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractBERTModel( AbstractModel):\n",
        "    def __init__(self, model_card, tokenizer, num_emotions, max_num_utterances, device, gru_hidden_size = None, freeze_embedding_layer=False):\n",
        "        super(AbstractBERTModel, self).__init__(num_emotions, max_num_utterances, device)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Load pre-trained BERT model and tokenizer\n",
        "        self.bert_model = BertModel.from_pretrained(model_card).to(device)\n",
        "\n",
        "        if freeze_embedding_layer:\n",
        "            for param in self.bert_model.embeddings.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            for param in self.bert_model.embeddings.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        self.representation_length = self.bert_model.config.hidden_size\n",
        "\n",
        "        self.emotion_classifier = nn.Sequential(\n",
        "            nn.Linear(self.representation_length, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_emotions)\n",
        "        )\n",
        "        self.gru_hidden_size  = gru_hidden_size\n",
        "        self.trigger_prediction = self._create_trigger_module()\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def _create_trigger_module(self):\n",
        "        pass\n",
        "\n",
        "    def bert_forward(self, input_ids, attention_mask):\n",
        "        bert_output_1 = self.bert_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        # Extract [SEP] token representations\n",
        "        sep_indices = (input_ids == self.tokenizer.sep_token_id).nonzero()\n",
        "        batch_size = input_ids.shape[0]\n",
        "        sep_representations = torch.zeros((batch_size, self.max_num_utterances, self.representation_length)).to(self.device)\n",
        "        dialogue_masks = torch.zeros((batch_size, self.max_num_utterances)).to(self.device)\n",
        "        for idx in range(batch_size):\n",
        "            sep_indices_idx = sep_indices[sep_indices[:,0] == idx][:,1]\n",
        "            sep_indices_idx_range = range(len(sep_indices_idx))\n",
        "            dialogue_masks[idx, sep_indices_idx_range] = 1\n",
        "            sep_representations[idx, sep_indices_idx_range, :] = bert_output_1[idx, sep_indices_idx, :]\n",
        "        return sep_representations, dialogue_masks"
      ],
      "metadata": {
        "id": "uHg7H0nj0gMJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NON SO SE IL FATTO CHE MOLTIPLICO CON LA DIALOGUE_MASK DIA PROBLEMI PER IL BACKWARD STEP, MAGARI E' EVITABILE. ANCHE NON MOLTIPLICANDO HO LO STESSO RISULTATO."
      ],
      "metadata": {
        "id": "RhM0tjT4v57A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBERTModel(AbstractBERTModel):\n",
        "    def __init__(self, model_card, tokenizer, num_emotions, max_num_utterances, device, gru_hidden_size, freeze_embedding_layer=False):\n",
        "        super(CustomBERTModel, self).__init__(model_card, tokenizer, num_emotions, max_num_utterances, device, gru_hidden_size, freeze_embedding_layer)\n",
        "        self.linear_trigger = nn.Linear(self.gru_hidden_size * 2, 1)\n",
        "\n",
        "\n",
        "    def _create_trigger_module(self):\n",
        "        return nn.GRU(\n",
        "            input_size=self.representation_length + num_emotions + 1,\n",
        "            hidden_size=self.gru_hidden_size,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "    def forward(self, token_type_ids, input_ids, attention_mask, _, __):\n",
        "        sep_representations, dialogue_masks = self.bert_forward(input_ids, attention_mask)\n",
        "        emotion_predictions = (self.emotion_classifier(sep_representations) * dialogue_masks.unsqueeze(-1)).to(self.device)\n",
        "        concatenated_input = torch.cat([sep_representations, emotion_predictions, token_type_ids.unsqueeze(-1)], dim=-1).to(self.device)\n",
        "        trigger_output, _ = self.trigger_prediction(concatenated_input)\n",
        "        trigger_output_single_value = self.linear_trigger(trigger_output.to(self.device)).squeeze(-1).to(self.device)\n",
        "        trigger_output_sigmoid = (torch.sigmoid(trigger_output_single_value) * dialogue_masks).to(self.device)\n",
        "        return emotion_predictions, trigger_output_sigmoid, dialogue_masks"
      ],
      "metadata": {
        "id": "F5s3MoVW0kEJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineBERTModel(AbstractBERTModel):\n",
        "    def __init__(self, model_card, tokenizer, num_emotions, max_num_utterances, device, freeze_embedding_layer=False):\n",
        "        super(BaselineBERTModel, self).__init__(model_card, tokenizer, num_emotions, max_num_utterances, device, freeze_embedding_layer)\n",
        "\n",
        "    def _create_trigger_module(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.representation_length, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, _, input_ids, attention_mask, __, ___):\n",
        "        sep_representations, dialogue_masks = self.bert_forward(input_ids, attention_mask)\n",
        "        emotion_predictions = (self.emotion_classifier(sep_representations) * dialogue_masks.unsqueeze(-1)).to(self.device)\n",
        "        trigger_output = (self.trigger_prediction(sep_representations) * dialogue_masks.unsqueeze(-1)).to(self.device)\n",
        "        trigger_output = trigger_output.squeeze(dim=-1).to(self.device)\n",
        "        return emotion_predictions, trigger_output, dialogue_masks"
      ],
      "metadata": {
        "id": "AvvN6rNw01fL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoGradClassifier(AbstractModel):\n",
        "    def __init__(self, num_emotions , max_num_utterances, device):\n",
        "        super(NoGradClassifier, self).__init__(num_emotions, max_num_utterances, device)\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_emotion_index(self):\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_trigger(self):\n",
        "      pass\n",
        "\n",
        "    def _populate_dialogue_masks(_, dialogue_masks, emotions):\n",
        "        for batch_idx, dialogue in enumerate(emotions):\n",
        "           for utterance_idx, token in enumerate(dialogue):\n",
        "              if np.any(token.cpu().numpy()):\n",
        "                dialogue_masks[batch_idx, utterance_idx] = 1\n",
        "              else:\n",
        "                dialogue_masks[batch_idx, utterance_idx] = 0\n",
        "\n",
        "    def forward(self, _, __, ___, emotions, triggers):\n",
        "        batch_size = emotions.shape[0]\n",
        "        dialogue_masks = torch.zeros((batch_size, self.max_num_utterances)).to(self.device)\n",
        "        self._populate_dialogue_masks(dialogue_masks, emotions)\n",
        "        emotion_pred = torch.zeros((batch_size, self.max_num_utterances, self.num_emotions)).to(self.device)\n",
        "        trigger_pred = torch.zeros((batch_size, self.max_num_utterances)).to(self.device)\n",
        "        for batch_idx, dialogue in enumerate(emotions):\n",
        "           for utterance_idx, _ in enumerate(dialogue):\n",
        "               active_index = self.generate_emotion_index()\n",
        "               one_hot_encoding = np.zeros(self.num_emotions)\n",
        "               one_hot_encoding[active_index] = 1\n",
        "               emotion_pred[batch_idx, utterance_idx, :] = torch.tensor(one_hot_encoding)\n",
        "               trigger_pred[batch_idx, utterance_idx] = self.generate_trigger()\n",
        "        emotion_pred = (emotion_pred * dialogue_masks.unsqueeze(-1)).to(self.device)\n",
        "        trigger_pred = (trigger_pred * dialogue_masks).to(self.device)\n",
        "        return emotion_pred, trigger_pred, dialogue_masks\n"
      ],
      "metadata": {
        "id": "K_Tp7ZkMJS22"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomClassifier(NoGradClassifier):\n",
        "    def __init__(self, num_emotions , max_num_utterances, device, seed):\n",
        "        super(RandomClassifier, self).__init__(num_emotions, max_num_utterances, device)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def generate_emotion_index(self):\n",
        "        return np.random.randint(self.num_emotions)\n",
        "\n",
        "    def generate_trigger(self):\n",
        "        return random.uniform(0, 1)"
      ],
      "metadata": {
        "id": "cE_5x6jffnAV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MajorityClassifier(NoGradClassifier):\n",
        "    def __init__(self, num_emotions , max_num_utterances,device, emotions, triggers):\n",
        "        super(MajorityClassifier, self).__init__(num_emotions, max_num_utterances, device)\n",
        "        self.majority_emotion_index = self._compute_majority_emotion_index(emotions)\n",
        "        self.majority_trigger = self._compute_majority_trigger(triggers)\n",
        "\n",
        "    def _compute_majority_emotion_index(self,emotions):\n",
        "        flattened_emotions = self._flatten(emotions)\n",
        "        flattened_emotions = torch.argmax(flattened_emotions, dim = -1)\n",
        "        return mode(flattened_emotions)\n",
        "\n",
        "    def _compute_majority_trigger(self, triggers):\n",
        "        flattened_triggers = self._flatten(triggers)\n",
        "        return mode(flattened_triggers)\n",
        "\n",
        "    def _flatten(self, data):\n",
        "        flattened_array = []\n",
        "        for dialogue in data:\n",
        "          for element in dialogue:\n",
        "            flattened_array.append(element)\n",
        "        return torch.tensor(flattened_array).to(self.device)\n",
        "\n",
        "    def generate_emotion_index(self):\n",
        "        return self.majority_emotion_index\n",
        "\n",
        "    def generate_trigger(self):\n",
        "        return self.majority_trigger"
      ],
      "metadata": {
        "id": "jGFK2R8OKvWT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pad(batch_idx, target, predictions, dialogue_length):\n",
        "        target_nopad = target[batch_idx, :dialogue_length]\n",
        "        predictions_nopad = predictions[batch_idx, :dialogue_length]\n",
        "        return target_nopad, predictions_nopad\n",
        "\n",
        "def remove_pad_predictions(batch_idx, emotions, emotion_predictions, triggers, trigger_predictions, dialogue_mask):\n",
        "        dialogue_bool = (dialogue_mask[batch_idx] == 0)\n",
        "        dialogue_true = dialogue_bool.nonzero()\n",
        "        if len(dialogue_true) == 0:\n",
        "          dialogue_length = len(dialogue_mask[batch_idx])\n",
        "        else:\n",
        "           dialogue_length = (dialogue_mask[batch_idx] == 0).nonzero()[0][0].item()\n",
        "        emotions_nopad, emotion_pred_nopad = remove_pad(batch_idx, emotions, emotion_predictions, dialogue_length)\n",
        "        triggers_nopad, trigger_pred_nopad = remove_pad(batch_idx, triggers, trigger_predictions, dialogue_length)\n",
        "        return emotions_nopad, emotion_pred_nopad, triggers_nopad, trigger_pred_nopad\n",
        "\n",
        "def update_metric_arrays(emotions_nopad, emotion_pred_nopad, triggers_nopad, triggers_pred_nopad,\n",
        "                    emotions_flat, emotions_pred_flat, triggers_flat, triggers_pred_flat,\n",
        "                    f1_seq_emotions, f1_seq_triggers):\n",
        "        emotions_flat.extend(emotions_nopad.tolist())\n",
        "        emotions_pred_flat.extend(emotion_pred_nopad.tolist())\n",
        "        triggers_flat.extend(triggers_nopad.tolist())\n",
        "        triggers_pred_flat.extend(triggers_pred_nopad.tolist())\n",
        "        f1_seq_emotions.append(f1_score(emotions_nopad.cpu().numpy(), emotion_pred_nopad.cpu().numpy(), average = 'micro'))\n",
        "        f1_seq_triggers.append(f1_score(triggers_nopad.cpu().numpy(), triggers_pred_nopad.cpu().numpy(), average = 'micro'))\n",
        "\n",
        "def get_metric_results(flattened_emotions, flattened_emotions_pred, flattened_triggers, flattened_triggers_pred, f1_sequence_emotions, f1_sequence_triggers):\n",
        "        avg_f1_sequence_emotion = sum(f1_sequence_emotions) / len(f1_sequence_emotions)\n",
        "        avg_f1_sequence_trigger = sum(f1_sequence_triggers) / len(f1_sequence_triggers)\n",
        "        f1_flattened_emotion = f1_score(flattened_emotions, flattened_emotions_pred, average='micro')\n",
        "        f1_flattened_trigger = f1_score(flattened_triggers, flattened_triggers_pred, average='micro')\n",
        "        return avg_f1_sequence_emotion, avg_f1_sequence_trigger, f1_flattened_emotion, f1_flattened_trigger\n",
        "\n",
        "def turn_into_greedy(emotions, emotion_pred, trigger_pred):\n",
        "        return torch.argmax(emotions, dim=-1), torch.argmax(emotion_pred, dim=-1), (trigger_pred > 0.5).float()"
      ],
      "metadata": {
        "id": "W-IpvSL4DEC_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel caso di CrossEntropyLoss, devo levare le PAD PREDICTIONS (non le voglio proprio no).\n"
      ],
      "metadata": {
        "id": "XKJemp0Zt9kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con torch.nn.BCELoss() lui si aspetta che le mie predictions siano GIA' SIGMOIDATE (tra 0 e 1). Nel caso in cui ho pad predictions e pad targets come nel caso qua sotto, il risultato e' giustamente 0. Il problema sta nel fatto che se devo fare AVERAGE di tutti i risultati, NON MI VA BENE che nel numero di esempi col quale poi andare a dividere RISULTI anche l'esempio PADDATO (che mi abbasserebbe la media, facendo aumentare il valore del denominatore)."
      ],
      "metadata": {
        "id": "oLgXgh5sukUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining():\n",
        "    def __init__(self, training_loader,validation_loader,test_loader,device: str,epochs=8,seed=42):\n",
        "        self.training_loader = training_loader\n",
        "        self.validation_loader = validation_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        self.seed = seed\n",
        "\n",
        "    def compute_loss(self, emotion_pred, emotions, trigger_pred, triggers, dialogue_mask):\n",
        "            N = 0\n",
        "            total_emotions_loss = 0\n",
        "            total_triggers_loss = 0\n",
        "            for batch_idx in range(emotion_pred.size(0)):\n",
        "                emotions_nopad, emotion_pred_nopad, triggers_nopad, triggers_pred_nopad = remove_pad_predictions(batch_idx,emotions,emotion_pred,triggers,trigger_pred,dialogue_mask)\n",
        "                discrete_emotions = torch.argmax(emotions_nopad, dim=-1).to(self.device)\n",
        "                N = N + triggers_nopad.size(0)\n",
        "                emotions_loss = torch.nn.CrossEntropyLoss(reduction ='sum')(emotion_pred_nopad, discrete_emotions)\n",
        "                triggers_loss = torch.nn.BCELoss(reduction ='sum')(triggers_pred_nopad, triggers_nopad)\n",
        "                total_emotions_loss += emotions_loss\n",
        "                total_triggers_loss += triggers_loss\n",
        "            mean_emotions_loss = total_emotions_loss/N\n",
        "            mean_triggers_loss = total_triggers_loss/N\n",
        "            loss = mean_emotions_loss + mean_triggers_loss\n",
        "            return loss.to(self.device)\n",
        "\n",
        "    def _compute_metrics(_, emotions, emotion_pred, triggers, trigger_pred, dialogue_mask,emotions_flat, emotions_pred_flat, triggers_flat, triggers_pred_flat, f1_sequence_emotions, f1_sequence_triggers):\n",
        "            emotions, emotion_pred , trigger_pred = turn_into_greedy(emotions, emotion_pred, trigger_pred)\n",
        "            for batch_idx in range(emotion_pred.size(0)):\n",
        "                emotions_nopad, emotion_pred_nopad, triggers_nopad, triggers_pred_nopad = remove_pad_predictions(batch_idx,emotions,emotion_pred,triggers,trigger_pred,dialogue_mask)\n",
        "                update_metric_arrays(emotions_nopad, emotion_pred_nopad, triggers_nopad, triggers_pred_nopad,emotions_flat, emotions_pred_flat, triggers_flat, triggers_pred_flat,f1_sequence_emotions, f1_sequence_triggers)\n",
        "\n",
        "    def _set_loop_info(self,  loop, loss, avg_loss, avg_f1_sequence_emotion, f1_flattened_emotion, avg_f1_sequence_trigger, f1_flattened_trigger):\n",
        "            loop.set_description(f'Train set.')\n",
        "            loop.set_postfix({'loss': f'{loss.item():.5}', 'loss_average': f'{avg_loss:.5}',\n",
        "                              'f1_sequence_emotion': f'{avg_f1_sequence_emotion:.5}',\n",
        "                              'f1_flattened_emotion': f'{f1_flattened_emotion:.5}',\n",
        "                              'f1_sequence_trigger': f'{avg_f1_sequence_trigger:.5}',\n",
        "                              'f1_flattened_trigger': f'{f1_flattened_trigger:.5}',})\n",
        "\n",
        "    def train_step(self, model: nn.Module, optimizer: Optimizer):\n",
        "        total_loss = 0\n",
        "        train_step = 0\n",
        "        emotions_flat, emotions_pred_flat ,triggers_flat, triggers_pred_flat, f1_sequence_emotions, f1_sequence_triggers = [],[],[],[],[],[]\n",
        "        loop = tqdm(enumerate(self.training_loader, 0), total=len(self.training_loader))\n",
        "        for _,data in loop:\n",
        "            optimizer.zero_grad()\n",
        "            train_step += 1\n",
        "            speakers, dialogues_ids, attention_masks, emotions, triggers = data\n",
        "            emotion_pred, trigger_pred, dialogue_mask =  model(speakers, dialogues_ids, attention_masks, emotions, triggers)\n",
        "            loss = self.compute_loss(emotion_pred, emotions, trigger_pred, triggers, dialogue_mask)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            self._compute_metrics(emotions, emotion_pred, triggers, trigger_pred, dialogue_mask,emotions_flat, emotions_pred_flat ,triggers_flat, triggers_pred_flat, f1_sequence_emotions, f1_sequence_triggers)\n",
        "            avg_f1_sequence_emotion, avg_f1_sequence_trigger, f1_flattened_emotion, f1_flattened_trigger = get_metric_results(emotions_flat, emotions_pred_flat ,triggers_flat, triggers_pred_flat,f1_sequence_emotions,f1_sequence_triggers)\n",
        "            total_loss += loss.item()\n",
        "            avg_loss = total_loss / train_step\n",
        "            self._set_loop_info(loop,  loss, avg_loss, avg_f1_sequence_emotion, f1_flattened_emotion, avg_f1_sequence_trigger, f1_flattened_trigger)\n",
        "        return avg_loss, avg_f1_sequence_emotion, f1_flattened_emotion, avg_f1_sequence_trigger, f1_flattened_trigger\n",
        "\n",
        "    def _populate_history(_, history, loss, f1seq_emotion, f1flat_emotion, f1seq_trigger, f1flat_trigger):\n",
        "        history['loss'].append(loss)\n",
        "        history['f1seq_emotion'].append(f1seq_emotion)\n",
        "        history['f1flat_emotion'].append(f1flat_emotion)\n",
        "        history['f1seq_trigger'].append(f1seq_trigger)\n",
        "        history['f1flat_trigger'].append(f1flat_trigger)\n",
        "\n",
        "    def train(self, model, optimizer, patience):\n",
        "            train_history = {'loss': [], 'f1seq_emotion': [], 'f1flat_emotion': [],\n",
        "                            'f1seq_trigger': [], 'f1flat_trigger': []}\n",
        "            val_history = {'loss': [], 'f1seq_emotion': [], 'f1flat_emotion': [],\n",
        "                          'f1seq_trigger': [], 'f1flat_trigger': []}\n",
        "            best_val_loss = float('inf')\n",
        "            for epoch in range(self.epochs):\n",
        "                model.train()\n",
        "                train_loss, f1seq_emotion_train, f1flat_emotion_train, f1seq_trigger_train, f1flat_trigger_train = self.train_step(model, optimizer)\n",
        "                self._populate_history(train_history, train_loss, f1seq_emotion_train, f1flat_emotion_train, f1seq_trigger_train, f1flat_trigger_train)\n",
        "                model.eval()\n",
        "                val_loss, f1seq_emotion_val, f1flat_emotion_val, f1seq_trigger_val, f1flat_trigger_val = self.evaluate(self.validation_loader, model)\n",
        "                self._populate_history(val_history, val_loss, f1seq_emotion_val, f1flat_emotion_val, f1seq_trigger_val, f1flat_trigger_val)\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    early_stopping_counter = 0\n",
        "                else:\n",
        "                    early_stopping_counter += 1\n",
        "                # Check if training should stop\n",
        "                if early_stopping_counter >= patience:\n",
        "                    print(f'Early stopping at epoch {epoch}...')\n",
        "                    break\n",
        "            return train_history, val_history\n",
        "\n",
        "    def test(self, model):\n",
        "               avg_loss, avg_f1_sequence_emotion, f1_flattened_emotion, avg_f1_sequence_trigger, f1_flattened_trigger =  self.evaluate(self.test_loader, model)\n",
        "               return {'f1_seq_emotion':avg_f1_sequence_emotion,\n",
        "                       'f1_flat_emotion':f1_flattened_emotion,\n",
        "                       'f1_seq_trigger': avg_f1_sequence_trigger,\n",
        "                       'f1_flat_trigger':f1_flattened_trigger}\n",
        "\n",
        "    def evaluate(self, dataloader, model):\n",
        "              total_loss = 0\n",
        "              evaluate_step = 0\n",
        "              emotions_flat, emotions_pred_flat ,triggers_flat, triggers_pred_flat, f1_sequence_emotions, f1_sequence_triggers = [],[],[],[],[],[]\n",
        "              with torch.no_grad():\n",
        "                  loop = tqdm(enumerate(dataloader, 0), total=len(dataloader))\n",
        "                  for _, data in loop:\n",
        "                      evaluate_step += 1\n",
        "                      speakers, dialogues_ids, attention_masks, emotions, triggers = data\n",
        "                      emotion_pred, trigger_pred, dialogue_mask =  model(speakers, dialogues_ids, attention_masks, emotions, triggers)\n",
        "                      loss = self.compute_loss(emotion_pred, emotions, trigger_pred, triggers, dialogue_mask)\n",
        "                      self._compute_metrics(emotions, emotion_pred, triggers, trigger_pred, dialogue_mask, emotions_flat, emotions_pred_flat, triggers_flat, triggers_pred_flat, f1_sequence_emotions, f1_sequence_triggers)\n",
        "                      total_loss += loss.item()\n",
        "                  avg_loss = 0.0\n",
        "                  avg_loss = total_loss / evaluate_step\n",
        "                  avg_f1_sequence_emotion, avg_f1_sequence_trigger, f1_flattened_emotion, f1_flattened_trigger = get_metric_results(emotions_flat,emotions_pred_flat,triggers_flat,triggers_pred_flat,f1_sequence_emotions,f1_sequence_triggers)\n",
        "              return avg_loss, avg_f1_sequence_emotion, f1_flattened_emotion, avg_f1_sequence_trigger, f1_flattened_trigger"
      ],
      "metadata": {
        "id": "dBzy0aqzfMBQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_trainer(dataframe: pd.core.frame.DataFrame,\n",
        "          device: str,\n",
        "          tokenizer: transformers.models.bert.tokenization_bert.BertTokenizer,\n",
        "          max_num_utterances: int,\n",
        "          batch_size: int,\n",
        "          seed):\n",
        "    df_train, df_val, df_test = split_dataframe(dataframe, seed = seed)\n",
        "    dataloader_train = create_dataloader(df_train, device, tokenizer, max_num_utterances, batch_size)\n",
        "    dataloader_val = create_dataloader(df_val, device, tokenizer, max_num_utterances, batch_size)\n",
        "    dataloader_test = create_dataloader(df_test, device, tokenizer, max_num_utterances, batch_size)\n",
        "    trainer = CustomTraining(dataloader_train, dataloader_val, dataloader_test, device)\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "Clm_mx58HdLO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value: int):\n",
        "    random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)"
      ],
      "metadata": {
        "id": "v2fXDRXleAQ8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_descriptive(dictionary, source=\"\"):\n",
        "    print(f\"Metrics results in evaluating {source}\")\n",
        "    for key, value in dictionary.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print('')"
      ],
      "metadata": {
        "id": "yLXZohTGpWUe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_card, do_lower_case=True)\n",
        "df = pd.read_json(dataset_path)\n",
        "emotions_onehotencoder = fit_onehotencoder(df['emotions'])\n",
        "df_standard, df_variation = build_dataframes(df, emotions_onehotencoder, tokenizer)\n",
        "num_emotions =  len(emotions_onehotencoder.classes_)\n",
        "max_num_utterances = df['utterances'].apply(len).max()\n",
        "trainer_standard = build_trainer(df_standard, device, tokenizer, max_num_utterances, batch_size, initial_seed)\n",
        "trainer_variation = build_trainer(df_variation, device, tokenizer, max_num_utterances, batch_size, initial_seed)\n",
        "majority_model = MajorityClassifier(num_emotions, max_num_utterances, device, df_standard['emotions'], df_standard['triggers']).to(device)\n",
        "random_model = RandomClassifier(num_emotions, max_num_utterances, device, seed = initial_seed).to(device)\n",
        "majority_model_metrics = trainer_standard.test(majority_model)"
      ],
      "metadata": {
        "id": "9wYjxSb_aNKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d481c4a5-4e35-45f1-f67e-7e8506c2a242"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-21-a2faa6cf7061>:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  return torch.tensor(flattened_array).to(self.device)\n",
            "100%|██████████| 200/200 [00:02<00:00, 67.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_model_metrics = trainer_standard.test(random_model)\n",
        "print_descriptive(majority_model_metrics, source= 'majority model')\n",
        "print_descriptive(random_model_metrics, source= 'random model')\n",
        "\n",
        "model_names = ['bert freezed model', 'bert unfreezed model', 'custom model']\n",
        "trainable_models_metrics = {model_names[0]: {'f1_seq_emotion': [], 'f1_flat_emotion': [],'f1_seq_trigger': [], 'f1_flat_trigger': []},\n",
        "                            model_names[1]: {'f1_seq_emotion': [], 'f1_flat_emotion': [],'f1_seq_trigger': [], 'f1_flat_trigger': []},\n",
        "                            model_names[2]: {'f1_seq_emotion': [], 'f1_flat_emotion': [],'f1_seq_trigger': [], 'f1_flat_trigger': []}}\n",
        "for seed in seeds:\n",
        "  set_seed(seed)\n",
        "  custom_model = CustomBERTModel(model_card, tokenizer, num_emotions, max_num_utterances, device,  gru_hidden_size = 256).to(device)\n",
        "  bert_freezed_model = BaselineBERTModel(model_card, tokenizer, num_emotions, max_num_utterances, device, freeze_embedding_layer = True).to(device)\n",
        "  bert_unfreezed_model = BaselineBERTModel(model_card, tokenizer, num_emotions, max_num_utterances, device, freeze_embedding_layer = False).to(device)\n",
        "  optimizer_lambda = lambda m: optim.Adam([ param for param in m.parameters() if param.requires_grad == True], lr=0.0015)\n",
        "  trainable_models = [bert_freezed_model, bert_unfreezed_model, custom_model]\n",
        "  trainers = [trainer_variation, trainer_variation, trainer_standard]\n",
        "  for idx, (model, trainer) in enumerate(zip(trainable_models, trainers)):\n",
        "    print('Training model {model}, on seed {seed_value} number {number}'.format(model = model_names[idx], seed_value = seed, number = idx))\n",
        "    train_history, val_history = trainer.train(model, optimizer_lambda(model), patience)\n",
        "    model_metrics = trainer.test(model)\n",
        "    print_descriptive(model_metrics, source=model_names[idx])\n",
        "    trainable_models_metrics[model_names[idx]]['f1_seq_emotion'].append(model_metrics['f1_seq_emotion'])\n",
        "    trainable_models_metrics[model_names[idx]]['f1_flat_emotion'].append(model_metrics['f1_flat_emotion'])\n",
        "    trainable_models_metrics[model_names[idx]]['f1_seq_trigger'].append(model_metrics['f1_seq_trigger'])\n",
        "    trainable_models_metrics[model_names[idx]]['f1_flat_trigger'].append(model_metrics['f1_flat_trigger'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "CjVqVjuJFMjk",
        "outputId": "caa61927-b2c8-4069-da07-7345f120105c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 69.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics results in evaluating majority model\n",
            "f1_seq_emotion: 0.4190589023568227\n",
            "f1_flat_emotion: 0.43461873006668594\n",
            "f1_seq_trigger: 0.7968414874529044\n",
            "f1_flat_trigger: 0.8428530008698172\n",
            "\n",
            "Metrics results in evaluating random model\n",
            "f1_seq_emotion: 0.1319801577309487\n",
            "f1_flat_emotion: 0.13105247897941433\n",
            "f1_seq_trigger: 0.5201107915184041\n",
            "f1_flat_trigger: 0.5134821687445636\n",
            "\n",
            "Training model bert freezed model, on seed 42 number 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train set.: 100%|██████████| 1600/1600 [06:15<00:00,  4.26it/s, loss=2.0194, loss_average=2.0967, f1_sequence_emotion=0.42664, f1_flattened_emotion=0.43525, f1_sequence_trigger=0.8, f1_flattened_trigger=0.83987]\n",
            "100%|██████████| 200/200 [00:10<00:00, 19.57it/s]\n",
            "Train set.: 100%|██████████| 1600/1600 [06:16<00:00,  4.26it/s, loss=2.0659, loss_average=2.081, f1_sequence_emotion=0.4269, f1_flattened_emotion=0.43568, f1_sequence_trigger=0.80055, f1_flattened_trigger=0.84019]\n",
            "100%|██████████| 200/200 [00:10<00:00, 19.55it/s]\n",
            "Train set.: 100%|██████████| 1600/1600 [06:16<00:00,  4.25it/s, loss=1.8757, loss_average=2.0804, f1_sequence_emotion=0.4269, f1_flattened_emotion=0.43568, f1_sequence_trigger=0.80055, f1_flattened_trigger=0.84019]\n",
            "100%|██████████| 200/200 [00:10<00:00, 19.57it/s]\n",
            "Train set.:  11%|█         | 169/1600 [00:35<04:57,  4.82it/s, loss=1.8695, loss_average=2.0755, f1_sequence_emotion=0.42883, f1_flattened_emotion=0.42942, f1_sequence_trigger=0.80084, f1_flattened_trigger=0.84195]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f945a7ba7052>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model {model}, on seed {seed_value} number {number}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint_descriptive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-99856323179b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, optimizer, patience)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1seq_emotion_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1flat_emotion_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1seq_trigger_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1flat_trigger_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_populate_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1seq_emotion_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1flat_emotion_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1seq_trigger_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1flat_trigger_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-99856323179b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, model, optimizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriggers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigger_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialogue_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotions_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions_pred_flat\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtriggers_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriggers_pred_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_sequence_emotions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_sequence_triggers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mavg_f1_sequence_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_f1_sequence_trigger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_flattened_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_flattened_trigger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metric_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotions_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions_pred_flat\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtriggers_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriggers_pred_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_sequence_emotions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_sequence_triggers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-2445e47f7900>\u001b[0m in \u001b[0;36mget_metric_results\u001b[0;34m(flattened_emotions, flattened_emotions_pred, flattened_triggers, flattened_triggers_pred, f1_sequence_emotions, f1_sequence_triggers)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mavg_f1_sequence_trigger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_sequence_triggers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_sequence_triggers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mf1_flattened_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_emotions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_emotions_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mf1_flattened_trigger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_triggers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_triggers_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mavg_f1_sequence_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_f1_sequence_trigger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_flattened_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_flattened_trigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.66666667\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m0.66666667\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \"\"\"\n\u001b[0;32m-> 1146\u001b[0;31m     return fbeta_score(\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \"\"\"\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1288\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m     MCM = multilabel_confusion_matrix(\n\u001b[0m\u001b[1;32m   1578\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    487\u001b[0m             [1, 2]]])\n\u001b[1;32m    488\u001b[0m     \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \"\"\"\n\u001b[1;32m   1178\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m     y = check_array(\n\u001b[0m\u001b[1;32m   1180\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUGyHz-c0w1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_models_metrics"
      ],
      "metadata": {
        "id": "7RuYEZRk-RwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "METRICS COMPARISON\n"
      ],
      "metadata": {
        "id": "mqqnb_JQqSSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(train_history, val_history, metric_names, metric_labels):\n",
        "    epochs = range(1, len(train_history['loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for train_metric, val_metric, label in zip(metric_names[::2], metric_names[1::2], metric_labels):\n",
        "        plt.plot(epochs, train_history[train_metric], label=f'Training {label}', linestyle='-', linewidth=2)\n",
        "        plt.plot(epochs, val_history[val_metric], label=f'Validation {label}', linestyle='--', linewidth=2)\n",
        "\n",
        "    plt.title('Training and Validation Metrics')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "taXsC4mN6tBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigger_metric_names = ['f1seq_trigger_train', 'f1flat_trigger_train', 'f1seq_trigger_val', 'f1flat_trigger_val']\n",
        "trigger_metric_labels = ['F1 Score - Seq Trigger', 'F1 Score - Flat Trigger']\n",
        "\n",
        "plot_metrics(train_history, val_history, trigger_metric_names, trigger_metric_labels)"
      ],
      "metadata": {
        "id": "pPnLQv9b6yew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, train_history['loss'], label='Training Loss', linestyle='-', linewidth=2)\n",
        "plt.plot(epochs, val_history['loss'], label='Validation Loss', linestyle='--', linewidth=2)"
      ],
      "metadata": {
        "id": "R1ffu_F085FY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}