{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFQcIOmv2DjF9qHtOMXm3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanieleBaiocco/NLPProject/blob/main/nlpproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from transformers import BertTokenizer\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "D8DvnzD7DP0M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mount_directory = \"/content/drive\"\n",
        "dataset_path = os.path.join(mount_directory, 'MyDrive/MELD/MELD_train_efr.json')\n",
        "model_card = 'bert-base-uncased'\n",
        "drive.mount(mount_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTPAnY4jWukx",
        "outputId": "9bc6e394-8d91-4689-d578-06a70f7dd52b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_onehotencoder(data: pd.core.series.Series) -> LabelBinarizer:\n",
        "  onehotencoder = LabelBinarizer()\n",
        "  data_flattened = np.concatenate(data.values)\n",
        "  onehotencoder.fit(data_flattened)\n",
        "  return onehotencoder"
      ],
      "metadata": {
        "id": "ENzi7RazbZeU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_to_order_of_occurrence(data: list) -> list:\n",
        "    # When a new entry is added, its value is computed using the lambda function\n",
        "    dict_order_of_occurrence = defaultdict(lambda: len(dict_order_of_occurrence)+1)\n",
        "    order_of_occurrence = [dict_order_of_occurrence[element] for element in data]\n",
        "    return order_of_occurrence"
      ],
      "metadata": {
        "id": "Pp1Tfr8PAVU8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_none_with_zero(data: list) -> list:\n",
        "    return [0 if x is None else x for x in data]"
      ],
      "metadata": {
        "id": "Qqer3ZIOE-td"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dialogue(dialogue_text: list, tokenizer) -> list:\n",
        "    tokenized_dialogue = [tokenizer.tokenize(utterance) + [tokenizer.sep_token] for utterance in dialogue_text]\n",
        "    tokenized_dialogue.insert(0, [tokenizer.cls_token])\n",
        "    flattened_tokens = [token for sublist in tokenized_dialogue for token in sublist]\n",
        "    #input_ids = tokenizer.convert_tokens_to_ids(flattened_tokens)\n",
        "    return flattened_tokens"
      ],
      "metadata": {
        "id": "ktvQEsNPE618"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding(x: list, max_x_length: int, pad_value, pad_length = None) -> list:\n",
        "    x_length = len(x)\n",
        "    num_pad_values = max_x_length - x_length\n",
        "    if pad_length == None:\n",
        "      padded_x = x + [pad_value] * num_pad_values\n",
        "    else:\n",
        "      pad_list = [pad_value] * pad_length\n",
        "      pad_lists = np.tile(pad_list, (num_pad_values, 1))\n",
        "      padded_x = np.concatenate((x, pad_lists), axis=0)\n",
        "    return padded_x"
      ],
      "metadata": {
        "id": "oA4NVL4vOd6k"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_card, do_lower_case=True)"
      ],
      "metadata": {
        "id": "OwnFTPgkDuHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363564f6-9b10-4b60-96ab-15b4cdfc8ffb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(dataset_path)\n",
        "column_names = ['emotions', 'speakers', 'triggers', 'dialogues', 'dialogues_ids', 'attention_masks']\n",
        "df_new = pd.DataFrame(columns = column_names)\n",
        "emotions_onehotencoder = fit_onehotencoder(df['emotions'])\n",
        "df_new['emotions'] = df['emotions'].apply(lambda x: emotions_onehotencoder.transform(x))\n",
        "df_new['speakers'] = df['speakers'].apply(lambda x: map_to_order_of_occurrence(x))\n",
        "df_new['triggers'] = df['triggers'].apply(lambda x: replace_none_with_zero(x))\n",
        "df_new['dialogues'] = df['utterances'].apply(lambda x: tokenize_dialogue(x, tokenizer))"
      ],
      "metadata": {
        "id": "emuZuvlwKAY1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe(df):\n",
        "    _folder = Path.cwd().joinpath(\"dataframes\")\n",
        "    if not _folder.exists():\n",
        "        _folder.mkdir(parents=True)\n",
        "\n",
        "    df_path = Path.joinpath(_folder, 'df_MELD_efr'+'.pkl')\n",
        "    df.to_pickle(df_path)"
      ],
      "metadata": {
        "id": "5g6CwpOlYgGN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dataframe(df_new)"
      ],
      "metadata": {
        "id": "4A218ugPYtQJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataframe():\n",
        "    df_path = 'dataframes/df_MELD_efr.pkl'\n",
        "    if not os.path.exists(df_path):\n",
        "        raise FileNotFoundError(\"{0} dataframe does not exist!\".format(df_path))\n",
        "\n",
        "    with open(df_path, 'rb') as file:\n",
        "        df = pickle.load(file)\n",
        "    return df"
      ],
      "metadata": {
        "id": "6mJwYXvlYlMK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataframe(orginal_df: pd.DataFrame, seed: int):\n",
        "    train, test_validation = train_test_split(orginal_df, test_size=0.2, random_state=seed)\n",
        "    validation, test = train_test_split(test_validation, test_size=0.5, random_state=seed)\n",
        "    return train.reset_index(drop=True), validation.reset_index(drop=True), test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "keNSPuCNqcWR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = split_dataframe(df_new, 42)"
      ],
      "metadata": {
        "id": "vfvBTQ4Sqx8g"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, speakers: pd.core.series.Series,\n",
        "                 dialogues:  pd.core.series.Series,\n",
        "                 emotions:  pd.core.series.Series,\n",
        "                 triggers:  pd.core.series.Series,\n",
        "                 pad_token: str):\n",
        "        self.max_dialogue_length = dialogues.apply(len).max()\n",
        "        self.dialogues = dialogues.apply(lambda x: add_padding(x, self.max_dialogue_length, pad_token))\n",
        "        self.dialogues_ids = self.dialogues.apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
        "        self.attention_masks = self.dialogues.apply(lambda x: [1 if token != pad_token else 0 for token in x])\n",
        "        self.speakers = speakers.apply(lambda x: add_padding(x, self.max_dialogue_length, pad_value=0))\n",
        "        num_emotion_classes = len(emotions[0][0])\n",
        "        self.emotions = emotions.apply(lambda x: add_padding(x, self.max_dialogue_length, pad_value = 0, pad_length = num_emotion_classes))\n",
        "        self.triggers = triggers.apply(lambda x: add_padding(x, self.max_dialogue_length, pad_value=0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        speakers = torch.tensor(self.speakers.iloc[idx], dtype=torch.float32)\n",
        "        dialogues_ids = torch.tensor(self.dialogues_ids.iloc[idx], dtype=torch.float32)\n",
        "        dialogues_masks =  torch.tensor(self.attention_masks.iloc[idx], dtype=torch.float32)\n",
        "        emotions = torch.tensor(self.emotions.iloc[idx], dtype=torch.float32)\n",
        "        triggers = torch.tensor(self.triggers.iloc[idx], dtype=torch.float32)\n",
        "        return  speakers, dialogues_ids, dialogues_masks, emotions, triggers"
      ],
      "metadata": {
        "id": "qrxDlf7pxa4d"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "torch_dataset = CustomDataset(speakers = df_train['speakers'],\n",
        "                              dialogues = df_train['dialogues'],\n",
        "                              emotions = df_train['emotions'],\n",
        "                              triggers = df_train['triggers'],\n",
        "                              pad_token = tokenizer.pad_token)\n",
        "dataloader_train = DataLoader(torch_dataset, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "lHW29c7BvHWW"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}